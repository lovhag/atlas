{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80945b6a",
   "metadata": {},
   "source": [
    "# Setup hyperparameter tuning of the Atlas fine-tuning on ParaRel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73a0ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7072ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_params = {\"batch_size\": [4, 32, 64],\n",
    "                   \"lr\": [(5e-5, 1e-5), (4e-5, 4e-5)],\n",
    "                   #\"train_steps\" covered in script\n",
    "                   \"retriever_temp\": [0.1, 0.01],\n",
    "                   \"n_context\": [10,20,30],\n",
    "                   \"training_data\": [(\"P138\",), (\"P138\",\"P127\"), (\"P138\",\"P127\",\"P1412\")]\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bdc7a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.1, 'n_context': 20, 'training_data': ('P138',)}\n",
      "{'batch_size': 4, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.01, 'n_context': 20, 'training_data': ('P138', 'P127', 'P1412')}\n",
      "{'batch_size': 4, 'lr': (4e-05, 4e-05), 'retriever_temp': 0.1, 'n_context': 30, 'training_data': ('P138', 'P127', 'P1412')}\n",
      "{'batch_size': 32, 'lr': (4e-05, 4e-05), 'retriever_temp': 0.01, 'n_context': 10, 'training_data': ('P138',)}\n",
      "{'batch_size': 64, 'lr': (4e-05, 4e-05), 'retriever_temp': 0.01, 'n_context': 10, 'training_data': ('P138',)}\n",
      "{'batch_size': 32, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.01, 'n_context': 10, 'training_data': ('P138', 'P127', 'P1412')}\n",
      "{'batch_size': 32, 'lr': (4e-05, 4e-05), 'retriever_temp': 0.1, 'n_context': 10, 'training_data': ('P138',)}\n",
      "{'batch_size': 64, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.1, 'n_context': 20, 'training_data': ('P138', 'P127', 'P1412')}\n",
      "{'batch_size': 64, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.01, 'n_context': 10, 'training_data': ('P138',)}\n",
      "{'batch_size': 32, 'lr': (4e-05, 4e-05), 'retriever_temp': 0.1, 'n_context': 20, 'training_data': ('P138',)}\n",
      "{'batch_size': 64, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.1, 'n_context': 20, 'training_data': ('P138', 'P127', 'P1412')}\n",
      "{'batch_size': 32, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.1, 'n_context': 10, 'training_data': ('P138',)}\n",
      "{'batch_size': 32, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.1, 'n_context': 30, 'training_data': ('P138', 'P127')}\n",
      "{'batch_size': 32, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.01, 'n_context': 10, 'training_data': ('P138', 'P127', 'P1412')}\n",
      "{'batch_size': 4, 'lr': (4e-05, 4e-05), 'retriever_temp': 0.1, 'n_context': 20, 'training_data': ('P138',)}\n",
      "{'batch_size': 4, 'lr': (4e-05, 4e-05), 'retriever_temp': 0.01, 'n_context': 20, 'training_data': ('P138',)}\n",
      "{'batch_size': 4, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.1, 'n_context': 20, 'training_data': ('P138', 'P127')}\n",
      "{'batch_size': 32, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.01, 'n_context': 20, 'training_data': ('P138', 'P127', 'P1412')}\n",
      "{'batch_size': 64, 'lr': (4e-05, 4e-05), 'retriever_temp': 0.01, 'n_context': 10, 'training_data': ('P138',)}\n",
      "{'batch_size': 32, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.01, 'n_context': 10, 'training_data': ('P138',)}\n",
      "{'batch_size': 64, 'lr': (4e-05, 4e-05), 'retriever_temp': 0.01, 'n_context': 30, 'training_data': ('P138', 'P127')}\n",
      "{'batch_size': 32, 'lr': (5e-05, 1e-05), 'retriever_temp': 0.01, 'n_context': 30, 'training_data': ('P138',)}\n"
     ]
    }
   ],
   "source": [
    "nbr_settings = 22\n",
    "setting_params_list = []\n",
    "for _ in range(nbr_settings):\n",
    "    setting_params = {}\n",
    "    for key, val in possible_params.items():\n",
    "        setting_params[key] = random.sample(val, 1)[0]\n",
    "    print(setting_params)\n",
    "    setting_params_list.append(setting_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80aacda",
   "metadata": {},
   "source": [
    "Create the corresponding run files starting from a template file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "259778ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "#SBATCH -p alvis\n",
      "#SBATCH -A SNIC2022-22-1040\n",
      "#SBATCH -N NULL\n",
      "#SBATCH --ntasks-per-node=4\n",
      "#SBATCH --gpus-per-node=A40:4\n",
      "#SBATCH --job-name=train-atlas-pararel-hyperparam-search-template\n",
      "#SBATCH -o /mimer/NOBACKUP/groups/snic2021-23-309/project-data/atlas/logs/pararel_train_hyperparam_search_template.out\n",
      "#SBATCH -t 0-04:00:00\n",
      "\n",
      "# COMMENT: this script relies on a previous passage encoding (load_index_path)\n",
      "\n",
      "set -eo pipefail\n",
      "\n",
      "module load PyTorch/1.11.0-foss-2021a-CUDA-11.3.1\n",
      "source venv/bin/activate\n",
      "\n",
      "size=base\n",
      "YEAR=${1:-\"2017\"}\n",
      "\n",
      "PASSAGES=\"data/corpora/wiki/enwiki-dec${YEAR}/text-list-100-sec.jsonl data/corpora/wiki/enwiki-dec${YEAR}/infobox.jsonl\"\n",
      "\n",
      "EXPERIMENT_NAME=template-${SLURM_JOB_ID}\n",
      "\n",
      "port=$(shuf -i 15000-16000 -n 1)\n",
      "TRAIN_FILES=\"/cephyr/users/lovhag/Alvis/projects/pararel/data/all_n1_atlas/NULL\"\n",
      "EVAL_FILES=\"/cephyr/users/lovhag/Alvis/projects/pararel/data/all_n1_atlas/P17_100.jsonl /cephyr/users/lovhag/Alvis/projects/pararel/data/all_n1_atlas/P101_100.jsonl /cephyr/users/lovhag/Alvis/projects/pararel/data/all_n1_atlas/P264_100.jsonl /cephyr/users/lovhag/Alvis/projects/pararel/data/all_n1_atlas/P449_100.jsonl\"\n",
      "PRETRAINED_MODEL=data/models/atlas/${size}\n",
      "SAVE_DIR=data/experiments/pararel_training_hyperparam_search/\n",
      "PRECISION=\"fp32\" # \"bf16\"\n",
      "\n",
      "srun python train.py \\\n",
      "    --shuffle \\\n",
      "    --train_retriever --query_side_retriever_training\\\n",
      "    --gold_score_mode ppmean \\\n",
      "    --use_gradient_checkpoint_reader \\\n",
      "    --use_gradient_checkpoint_retriever\\\n",
      "    --precision ${PRECISION} \\\n",
      "    --shard_optim --shard_grads \\\n",
      "    --temperature_gold 0.01 --temperature_score NULL \\\n",
      "    --refresh_index -1 \\\n",
      "    --target_maxlength 16 \\\n",
      "    --reader_model_type google/t5-${size}-lm-adapt \\\n",
      "    --dropout 0.1 \\\n",
      "    --lr NULL --lr_retriever NULL \\\n",
      "    --scheduler linear \\\n",
      "    --weight_decay 0.01 \\\n",
      "    --text_maxlength 512 \\\n",
      "    --model_path ${PRETRAINED_MODEL} \\\n",
      "    --train_data ${TRAIN_FILES} \\\n",
      "    --eval_data ${EVAL_FILES} \\\n",
      "    --per_gpu_batch_size 1 \\\n",
      "    --n_context NULL --retriever_n_context NULL \\\n",
      "    --name ${EXPERIMENT_NAME} \\\n",
      "    --checkpoint_dir ${SAVE_DIR} \\\n",
      "    --eval_freq 25 \\\n",
      "    --log_freq 4 \\\n",
      "    --total_steps 150 \\\n",
      "    --warmup_steps 20 \\\n",
      "    --save_freq 25 \\\n",
      "    --main_port $port \\\n",
      "    --write_results \\\n",
      "    --task qa \\\n",
      "    --index_mode flat \\\n",
      "    --passages ${PASSAGES}\\\n",
      "    --qa_prompt_format \"{question}\" \\\n",
      "    --load_index_path data/experiments/853398-base-templama-2017/saved_index  \\\n",
      "    --use_decoder_choices\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template_file_path = \"/cephyr/users/lovhag/Alvis/projects/atlas/alvis_scripts/pararel_training_hyperparam_search/train_template.sh\"\n",
    "\n",
    "template_file = \"\"\n",
    "with open(template_file_path) as f:\n",
    "    for line in f.readlines():\n",
    "        template_file = template_file + line\n",
    "print(template_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8380475",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_names = {\"batch_size\": [\"#SBATCH -N\"],\n",
    "                \"lr\": [\"--lr\", \"--lr_retriever\"],\n",
    "                \"retriever_temp\": [\"--temperature_score\"],\n",
    "                \"n_context\": [\"--n_context\", \"--retriever_n_context\"],\n",
    "                \"training_data\": [\"TRAIN_FILES='/cephyr/users/lovhag/Alvis/projects/pararel/data/all_n1_atlas/'\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2ed0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, setting_params in enumerate(setting_params_list):\n",
    "    exp_num = f\"{ix:02}\"\n",
    "    new_file = template_file.replace(\"template\", exp_num)\n",
    "    new_file = new_file.replace(\"#SBATCH -N NULL\", f\"#SBATCH -N {int(setting_params['batch_size']/4)}\")\n",
    "    new_file = new_file.replace(\"--lr NULL\", f\"--lr {setting_params['lr'][0]}\")\n",
    "    new_file = new_file.replace(\"--lr_retriever NULL\", f\"--lr_retriever {setting_params['lr'][1]}\")\n",
    "    new_file = new_file.replace(\"--temperature_score NULL\", f\"--temperature_score {setting_params['retriever_temp']}\")\n",
    "    new_file = new_file.replace(\"--n_context NULL\", f\"--n_context {setting_params['n_context']}\")\n",
    "    new_file = new_file.replace(\"--retriever_n_context NULL\", f\"--retriever_n_context {setting_params['n_context']}\")\n",
    "    \n",
    "    train_dir = \"/cephyr/users/lovhag/Alvis/projects/pararel/data/all_n1_atlas/\"\n",
    "    train_files = []\n",
    "    for train_file in setting_params[\"training_data\"]:\n",
    "        train_files.append(os.path.join(train_dir, train_file+\".jsonl\"))\n",
    "    replace_string = 'TRAIN_FILES=\"/cephyr/users/lovhag/Alvis/projects/pararel/data/all_n1_atlas/NULL\"'\n",
    "    new_file = new_file.replace(replace_string, f'TRAIN_FILES=\"{(\" \").join(train_files)}\"')\n",
    "    \n",
    "    filename = template_file_path.replace(\"template\", exp_num)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(new_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
